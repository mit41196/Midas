{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:33:25.964966Z",
     "start_time": "2019-04-03T07:32:39.165075Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:33:25.982659Z",
     "start_time": "2019-04-03T07:33:25.964966Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:33:26.076085Z",
     "start_time": "2019-04-03T07:33:25.992653Z"
    }
   },
   "outputs": [],
   "source": [
    "dimension = (28,28)\n",
    "input_shape = (28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T20:44:02.124599Z",
     "start_time": "2019-04-02T20:44:02.118604Z"
    }
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:48:24.394342Z",
     "start_time": "2019-04-03T07:48:24.388343Z"
    }
   },
   "outputs": [],
   "source": [
    "def augmentation(X_train, y_train, X_Test, y_test):\n",
    "    \n",
    "    generator = ImageDataGenerator(rotation_range=10, horizontal_flip=True, fill_mode=\"nearest\")\n",
    "    train_batches = generator.flow(X_train, y_train, batch_size=128)\n",
    "#     test_batches = generator.flow(X_Test, y_test, batch_size=256)\n",
    "    \n",
    "    return train_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:33:33.201873Z",
     "start_time": "2019-04-03T07:33:33.191877Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_network(inputs):\n",
    "    \n",
    "    '''--- Simple CNN Architecture ---'''\n",
    "    \n",
    "    # 2D Convolutional layer to extract features by applying filter on the given image\n",
    "    cnn = Sequential()\n",
    "    cnn.add(Conv2D(64, (3, 3), activation='relu', input_shape=inputs))\n",
    "    \n",
    "    # Max Pooling layer to reduce dimensionality and Dropout layer to prevent the model from overfitting\n",
    "    cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    cnn.add(Dropout(0.3))\n",
    "    \n",
    "    # Flatten the current Output produced by previous layers\n",
    "    cnn.add(Flatten())\n",
    "    cnn.add(Dense(128, activation=\"relu\"))\n",
    "    cnn.add(Dropout(0.25))\n",
    "    \n",
    "    # Dense layer to give 4 outputs as 4-class classification\n",
    "    cnn.add(Dense(4, activation=\"softmax\"))\n",
    "    \n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:33:34.680372Z",
     "start_time": "2019-04-03T07:33:34.343894Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('train_image.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    f.close()\n",
    "with open('train_label.pkl', 'rb') as f:\n",
    "    train_label = pickle.load(f)\n",
    "    f.close()\n",
    "with open('test_image.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert it into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:33:39.005049Z",
     "start_time": "2019-04-03T07:33:36.624752Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame(train_data)\n",
    "tests = pd.DataFrame(test_data)\n",
    "train_label = pd.DataFrame(train_label)\n",
    "train = pd.concat([train_data, train_label], axis=1, ignore_index=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:33:40.546507Z",
     "start_time": "2019-04-03T07:33:39.722631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replacing label 6 as 1 for simplicity of output\n",
    "uniq  = np.unique(train_label)\n",
    "train_lbl = train_label.replace(train_label[train_label[:]==6], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:33:41.832236Z",
     "start_time": "2019-04-03T07:33:41.758240Z"
    }
   },
   "outputs": [],
   "source": [
    "# Categorize the output labels and Train-Validation Sets are generated!\n",
    "y = LabelBinarizer().fit_transform(train_lbl)\n",
    "X_trains, X_tests, y_train, y_test = train_test_split(train_data, y, test_size=0.2, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping each array into image so it can fit in the CNN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:33:44.979943Z",
     "start_time": "2019-04-03T07:33:44.974945Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_trains.values\n",
    "X_test = X_tests.values\n",
    "X_train = X_train.reshape(X_train.shape[0], dimension[0], dimension[1],1)\n",
    "X_test = X_test.reshape(X_test.shape[0], dimension[0], dimension[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:34:03.413131Z",
     "start_time": "2019-04-03T07:34:03.390144Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize\n",
    "test = tests.values.reshape(tests.shape[0], dimension[0], dimension[1], 1)\n",
    "test = test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization of all the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:34:13.010734Z",
     "start_time": "2019-04-03T07:34:12.933726Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train/255.0\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment the train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:48:28.602798Z",
     "start_time": "2019-04-03T07:48:28.577801Z"
    }
   },
   "outputs": [],
   "source": [
    "train_batch = augmentation(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:34:17.166121Z",
     "start_time": "2019-04-03T07:34:16.703185Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1384576   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,385,732\n",
      "Trainable params: 1,385,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From c:\\python\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2885: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From c:\\python\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "cnn = build_network(input_shape)\n",
    "cnn.summary()\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "cnn.compile(metrics=[\"accuracy\"], optimizer = opt, loss= \"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model is trained on both types of data: Augmented and Without Augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:39:06.733145Z",
     "start_time": "2019-04-03T07:34:32.136934Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/20\n",
      "6400/6400 [==============================] - 15s 2ms/step - loss: 1.3772 - acc: 0.3341 - val_loss: 1.3606 - val_acc: 0.4738\n",
      "Epoch 2/20\n",
      "6400/6400 [==============================] - 14s 2ms/step - loss: 1.3201 - acc: 0.4777 - val_loss: 1.2665 - val_acc: 0.4800\n",
      "Epoch 3/20\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 1.1815 - acc: 0.5386 - val_loss: 1.0952 - val_acc: 0.5894\n",
      "Epoch 4/20\n",
      "6400/6400 [==============================] - 12s 2ms/step - loss: 1.0075 - acc: 0.6075 - val_loss: 0.9286 - val_acc: 0.6663\n",
      "Epoch 5/20\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 0.8833 - acc: 0.6475 - val_loss: 0.8257 - val_acc: 0.6863\n",
      "Epoch 6/20\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 0.8113 - acc: 0.6628 - val_loss: 0.7708 - val_acc: 0.6944\n",
      "Epoch 7/20\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 0.7735 - acc: 0.6698 - val_loss: 0.7553 - val_acc: 0.7006\n",
      "Epoch 8/20\n",
      "6400/6400 [==============================] - 15s 2ms/step - loss: 0.7465 - acc: 0.6809 - val_loss: 0.7123 - val_acc: 0.7063\n",
      "Epoch 9/20\n",
      "6400/6400 [==============================] - 16s 2ms/step - loss: 0.7259 - acc: 0.6897 - val_loss: 0.6941 - val_acc: 0.7081\n",
      "Epoch 10/20\n",
      "6400/6400 [==============================] - 18s 3ms/step - loss: 0.7066 - acc: 0.6958 - val_loss: 0.6774 - val_acc: 0.7188\n",
      "Epoch 11/20\n",
      "6400/6400 [==============================] - 15s 2ms/step - loss: 0.6852 - acc: 0.7047 - val_loss: 0.6703 - val_acc: 0.7163\n",
      "Epoch 12/20\n",
      "6400/6400 [==============================] - 14s 2ms/step - loss: 0.6887 - acc: 0.6997 - val_loss: 0.6673 - val_acc: 0.7044\n",
      "Epoch 13/20\n",
      "6400/6400 [==============================] - 12s 2ms/step - loss: 0.6712 - acc: 0.7075 - val_loss: 0.6613 - val_acc: 0.7063\n",
      "Epoch 14/20\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 0.6634 - acc: 0.7134 - val_loss: 0.6367 - val_acc: 0.7269\n",
      "Epoch 15/20\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 0.6586 - acc: 0.7139 - val_loss: 0.6358 - val_acc: 0.7275\n",
      "Epoch 16/20\n",
      "6400/6400 [==============================] - 12s 2ms/step - loss: 0.6436 - acc: 0.7214 - val_loss: 0.6308 - val_acc: 0.7319\n",
      "Epoch 17/20\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 0.6364 - acc: 0.7305 - val_loss: 0.6178 - val_acc: 0.7481\n",
      "Epoch 18/20\n",
      "6400/6400 [==============================] - 13s 2ms/step - loss: 0.6317 - acc: 0.7302 - val_loss: 0.6136 - val_acc: 0.7494\n",
      "Epoch 19/20\n",
      "6400/6400 [==============================] - 14s 2ms/step - loss: 0.6263 - acc: 0.7328 - val_loss: 0.6078 - val_acc: 0.7519\n",
      "Epoch 20/20\n",
      "6400/6400 [==============================] - 14s 2ms/step - loss: 0.6157 - acc: 0.7414 - val_loss: 0.6006 - val_acc: 0.7575\n"
     ]
    }
   ],
   "source": [
    "model = cnn.fit(X_train, y_train, batch_size=256, verbose=1, epochs=20, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:55:33.312199Z",
     "start_time": "2019-04-03T07:48:32.042760Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "50/50 [==============================] - 14s 286ms/step - loss: 0.6352 - acc: 0.7317 - val_loss: 0.5945 - val_acc: 0.7525\n",
      "Epoch 2/30\n",
      "50/50 [==============================] - 15s 293ms/step - loss: 0.6238 - acc: 0.7423 - val_loss: 0.5857 - val_acc: 0.7550\n",
      "Epoch 3/30\n",
      "50/50 [==============================] - 14s 286ms/step - loss: 0.6145 - acc: 0.7453 - val_loss: 0.5737 - val_acc: 0.7719\n",
      "Epoch 4/30\n",
      "50/50 [==============================] - 15s 290ms/step - loss: 0.6152 - acc: 0.7463 - val_loss: 0.5712 - val_acc: 0.7825\n",
      "Epoch 5/30\n",
      "50/50 [==============================] - 14s 276ms/step - loss: 0.5998 - acc: 0.7514 - val_loss: 0.5754 - val_acc: 0.7744\n",
      "Epoch 6/30\n",
      "50/50 [==============================] - 14s 275ms/step - loss: 0.6036 - acc: 0.7541 - val_loss: 0.5676 - val_acc: 0.7725\n",
      "Epoch 7/30\n",
      "50/50 [==============================] - 15s 294ms/step - loss: 0.5828 - acc: 0.7634 - val_loss: 0.5456 - val_acc: 0.7844\n",
      "Epoch 8/30\n",
      "50/50 [==============================] - 14s 275ms/step - loss: 0.5863 - acc: 0.7622 - val_loss: 0.5535 - val_acc: 0.7725\n",
      "Epoch 9/30\n",
      "50/50 [==============================] - 14s 280ms/step - loss: 0.5805 - acc: 0.7638 - val_loss: 0.5418 - val_acc: 0.7837\n",
      "Epoch 10/30\n",
      "50/50 [==============================] - 14s 286ms/step - loss: 0.5777 - acc: 0.7630 - val_loss: 0.5384 - val_acc: 0.7837\n",
      "Epoch 11/30\n",
      "50/50 [==============================] - 15s 308ms/step - loss: 0.5610 - acc: 0.7747 - val_loss: 0.5283 - val_acc: 0.7919\n",
      "Epoch 12/30\n",
      "50/50 [==============================] - 15s 295ms/step - loss: 0.5681 - acc: 0.7714 - val_loss: 0.5516 - val_acc: 0.7794\n",
      "Epoch 13/30\n",
      "50/50 [==============================] - 14s 279ms/step - loss: 0.5678 - acc: 0.7680 - val_loss: 0.5376 - val_acc: 0.7831\n",
      "Epoch 14/30\n",
      "50/50 [==============================] - 14s 274ms/step - loss: 0.5562 - acc: 0.7722 - val_loss: 0.5177 - val_acc: 0.7937\n",
      "Epoch 15/30\n",
      "50/50 [==============================] - 13s 264ms/step - loss: 0.5646 - acc: 0.7728 - val_loss: 0.5279 - val_acc: 0.7975\n",
      "Epoch 16/30\n",
      "50/50 [==============================] - 15s 298ms/step - loss: 0.5569 - acc: 0.7752 - val_loss: 0.5237 - val_acc: 0.7913\n",
      "Epoch 17/30\n",
      "50/50 [==============================] - 14s 287ms/step - loss: 0.5490 - acc: 0.7802 - val_loss: 0.5111 - val_acc: 0.8006\n",
      "Epoch 18/30\n",
      "50/50 [==============================] - 15s 295ms/step - loss: 0.5470 - acc: 0.7795 - val_loss: 0.5131 - val_acc: 0.7950\n",
      "Epoch 19/30\n",
      "50/50 [==============================] - 14s 272ms/step - loss: 0.5385 - acc: 0.7869 - val_loss: 0.5111 - val_acc: 0.7956\n",
      "Epoch 20/30\n",
      "50/50 [==============================] - 15s 303ms/step - loss: 0.5411 - acc: 0.7850 - val_loss: 0.5171 - val_acc: 0.7950\n",
      "Epoch 21/30\n",
      "50/50 [==============================] - 13s 263ms/step - loss: 0.5414 - acc: 0.7856 - val_loss: 0.5090 - val_acc: 0.8031\n",
      "Epoch 22/30\n",
      "50/50 [==============================] - 13s 268ms/step - loss: 0.5383 - acc: 0.7803 - val_loss: 0.5220 - val_acc: 0.7956\n",
      "Epoch 23/30\n",
      "50/50 [==============================] - 13s 267ms/step - loss: 0.5352 - acc: 0.7839 - val_loss: 0.4961 - val_acc: 0.8013\n",
      "Epoch 24/30\n",
      "50/50 [==============================] - 13s 262ms/step - loss: 0.5275 - acc: 0.7869 - val_loss: 0.4922 - val_acc: 0.8013\n",
      "Epoch 25/30\n",
      "50/50 [==============================] - 14s 271ms/step - loss: 0.5332 - acc: 0.7870 - val_loss: 0.5016 - val_acc: 0.7894\n",
      "Epoch 26/30\n",
      "50/50 [==============================] - 13s 263ms/step - loss: 0.5228 - acc: 0.7883 - val_loss: 0.5066 - val_acc: 0.8063\n",
      "Epoch 27/30\n",
      "50/50 [==============================] - 13s 262ms/step - loss: 0.5218 - acc: 0.7911 - val_loss: 0.4865 - val_acc: 0.8050\n",
      "Epoch 28/30\n",
      "50/50 [==============================] - 14s 272ms/step - loss: 0.5192 - acc: 0.7903 - val_loss: 0.4999 - val_acc: 0.8025\n",
      "Epoch 29/30\n",
      "50/50 [==============================] - 15s 301ms/step - loss: 0.5142 - acc: 0.7867 - val_loss: 0.4893 - val_acc: 0.8081\n",
      "Epoch 30/30\n",
      "50/50 [==============================] - 14s 276ms/step - loss: 0.5136 - acc: 0.7928 - val_loss: 0.4858 - val_acc: 0.8069\n"
     ]
    }
   ],
   "source": [
    "model = cnn.fit_generator(train_batch, steps_per_epoch=6400//128, epochs=30,validation_data=(X_test, y_test), validation_steps=1600//128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Performance on the Whole Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:58:22.203694Z",
     "start_time": "2019-04-03T07:58:18.898171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.46025737941265105\n",
      "Training Accuracy: 0.8196875\n"
     ]
    }
   ],
   "source": [
    "score = cnn.evaluate(X_train, y_train, verbose=0)\n",
    "print('Training Loss:', score[0])\n",
    "print('Training Accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Classes for the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:58:34.652582Z",
     "start_time": "2019-04-03T07:58:33.638062Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_classes = cnn.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:58:38.937811Z",
     "start_time": "2019-04-03T07:58:38.904829Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace with its original label i.e., 1 is replaced with 6\n",
    "x = pd.DataFrame(predicted_classes)\n",
    "u = x.replace(x[x[:]==1], 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-03T07:59:42.409318Z",
     "start_time": "2019-04-03T07:59:42.385321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write results into csv file\n",
    "u.to_csv(\"Meet_Shah.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
